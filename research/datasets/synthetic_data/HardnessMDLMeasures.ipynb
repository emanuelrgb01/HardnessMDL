{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad5f3b2a",
   "metadata": {},
   "source": [
    "# HardnessMDL: Instance Hardness Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path_to_research = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "if path_to_research not in sys.path:\n",
    "    sys.path.insert(0, path_to_research)\n",
    "\n",
    "print(f\"sys.path: {path_to_research}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bfc916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dceb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_plots(folder_path: str, num_datasets: int = 4):\n",
    "  fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "  fig.suptitle(f'Scatter Plots of {folder_path}')\n",
    "  \n",
    "  for i in range(num_datasets):\n",
    "    df = pd.read_csv(f\"{folder_path}test{i+1}.csv\")\n",
    "    sns.scatterplot(ax=axes[i], data=df, x='X', y='Y', hue='class')\n",
    "    axes[i].set_title(f'DataFrame {i+1}')\n",
    "  \n",
    "  plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def transform_dl_string_to_columns(df: pd.DataFrame) -> pd.DataFrame: \n",
    "    df_copy = df.copy() # without side-effects\n",
    "\n",
    "    df_copy['is_error'] = (df_copy['true_label'] != df_copy['label']).astype(int)\n",
    "    \n",
    "    df_copy[\"description_lengths_list\"] = df_copy[\"description_lengths_unnormalized\"].apply(\n",
    "        lambda x: [float(i) for i in x.strip(\"[]\").split(\" \") if i]\n",
    "    )\n",
    "\n",
    "    description_lengths_columns = pd.DataFrame(\n",
    "        df_copy[\"description_lengths_list\"].tolist(),\n",
    "        columns=[f\"L_{i}\" for i in range(len(df_copy[\"description_lengths_list\"].iloc[0]))],\n",
    "        index=df_copy.index\n",
    "    )\n",
    "\n",
    "    return pd.concat([df_copy.drop(columns=[\"description_lengths_list\"]), description_lengths_columns], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def compute_hardness_measures(df):\n",
    "    \"\"\"\n",
    "    Calculates a suite of instance hardness measures based on the model's\n",
    "    description length (L_*) outputs.\n",
    "\n",
    "    All measures are standardized to [0, 1], where 0.0 is easy and 1.0 is hard.\n",
    "    \"\"\"\n",
    "    l_cols = [col for col in df.columns if col.startswith('L_')]\n",
    "    n_classes = len(l_cols)\n",
    "    \n",
    "    def row_hardness(row):\n",
    "        description_lengths = np.array([row[col] for col in l_cols])\n",
    "        true_label_idx = int(row['true_label'])\n",
    "\n",
    "        epsilon = 1e-9\n",
    "        description_lengths[description_lengths <= 0] = epsilon\n",
    "        \n",
    "        dl_true = description_lengths[true_label_idx]\n",
    "        other_dls = np.delete(description_lengths, true_label_idx)\n",
    "        dl_range = np.max(description_lengths) - np.min(description_lengths)\n",
    "        \n",
    "        r_min = min(1.0, dl_true / (np.min(other_dls) + epsilon))\n",
    "        r_med = min(1.0, dl_true / (np.mean(other_dls) + epsilon))\n",
    "        rel_pos = (dl_true - np.min(description_lengths)) / (dl_range + epsilon)\n",
    "\n",
    "        \n",
    "        probs = np.exp(-(description_lengths - np.min(description_lengths)))\n",
    "        probs /= np.sum(probs)\n",
    "        \n",
    "        pseudo_prob = 1.0 - probs[true_label_idx]\n",
    "\n",
    "        max_entropy = np.log2(n_classes) if n_classes > 1 else 1.0\n",
    "        norm_entropy = entropy(probs, base=2) / (max_entropy + epsilon)\n",
    "        \n",
    "        sorted_dls = np.sort(description_lengths)\n",
    "        if len(sorted_dls) > 1:\n",
    "            margin = sorted_dls[1] - sorted_dls[0]\n",
    "            description_lenght_margin = 1.0 - (margin / (np.sum(sorted_dls) + epsilon))\n",
    "        else:\n",
    "            description_lenght_margin = 0.0\n",
    "\n",
    "        \n",
    "        description_lenght_true_cost = dl_true / (np.sum(description_lengths) + epsilon)\n",
    "        \n",
    "        ideal_dist = np.full(n_classes, epsilon)\n",
    "        ideal_dist[true_label_idx] = 1.0\n",
    "\n",
    "        ideal_dist /= np.sum(ideal_dist)\n",
    "        \n",
    "        kl_div = entropy(pk=ideal_dist, qk=probs, base=2)\n",
    "\n",
    "        kullback_leibler_divergence = 1.0 / (1.0 + np.exp(-kl_div))\n",
    "\n",
    "        return pd.Series({\n",
    "            'r_min': r_min,\n",
    "            'r_med': r_med,\n",
    "            'relative_position': rel_pos,\n",
    "            'pseudo_probability': pseudo_prob,\n",
    "            'normalized_entropy': norm_entropy,\n",
    "            'description_lenght_margin': description_lenght_margin,\n",
    "            'description_lenght_true_cost': description_lenght_true_cost,\n",
    "            'kullback_leibler_divergence': kullback_leibler_divergence\n",
    "        })\n",
    "\n",
    "    return df.join(df.apply(row_hardness, axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hardness(dfs: list, hardness_measure: str, save_path: str = None, show_plot: bool = False):\n",
    "  \n",
    "  df_meta_feats_dict = {}\n",
    "  for i, df in enumerate(dfs):\n",
    "      df_meta_feats_dict[f'DataFrame {i+1}'] = df\n",
    "\n",
    "  fig = make_subplots(rows=1, cols=4, subplot_titles=(\n",
    "      \"DataFrame 1\",\n",
    "      \"DataFrame 2\",\n",
    "      \"DataFrame 3\",\n",
    "      \"DataFrame 4\"\n",
    "  ))\n",
    "\n",
    "  for i, df in enumerate(dfs):\n",
    "      df_name = f'DataFrame {i+1}'\n",
    "      dcp_values = df_meta_feats_dict[df_name][hardness_measure]\n",
    "\n",
    "      fig.add_trace(go.Scattergl(x=df[\"X\"], y=df[\"Y\"], mode='markers',\n",
    "                              marker=dict(color=dcp_values,\n",
    "                                          colorscale='viridis',\n",
    "                                          cmin=0, cmax=1, \n",
    "                                          showscale=True if i == 3 else False, \n",
    "                                          colorbar=dict(title=hardness_measure, x=1.02)),\n",
    "                              name=df_name),\n",
    "                    row=1, col=i+1)\n",
    "\n",
    "  fig.update_layout(title_text=f\"Scatter Plots of DataFrames 1 to 4: {hardness_measure}\", showlegend=False)\n",
    "  if show_plot:\n",
    "        fig.show()\n",
    "  if save_path is not None:\n",
    "        fig.write_image(save_path, width=1600, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d015364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  return compute_hardness_measures(transform_dl_string_to_columns(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder_plots(\"two_classes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36656279",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder_plots(\"three_classes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e63003",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder_plots(\"five_classes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fe424",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['X', 'Y']\n",
    "label_column = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a240e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_1 = process_df(pd.read_csv('two_classes/results/test1_description_lenght_unnormalized.csv'))\n",
    "df2_2 = process_df(pd.read_csv('two_classes/results/test2_description_lenght_unnormalized.csv'))\n",
    "df2_3 = process_df(pd.read_csv('two_classes/results/test3_description_lenght_unnormalized.csv'))\n",
    "df2_4 = process_df(pd.read_csv('two_classes/results/test4_description_lenght_unnormalized.csv'))\n",
    "\n",
    "df3_1 = process_df(pd.read_csv('three_classes/results/test1_description_lenght_unnormalized.csv'))\n",
    "df3_2 = process_df(pd.read_csv('three_classes/results/test2_description_lenght_unnormalized.csv'))\n",
    "df3_3 = process_df(pd.read_csv('three_classes/results/test3_description_lenght_unnormalized.csv'))\n",
    "df3_4 = process_df(pd.read_csv('three_classes/results/test4_description_lenght_unnormalized.csv'))\n",
    "\n",
    "df5_1 = process_df(pd.read_csv('five_classes/results/test1_description_lenght_unnormalized.csv'))\n",
    "df5_2 = process_df(pd.read_csv('five_classes/results/test2_description_lenght_unnormalized.csv'))\n",
    "df5_3 = process_df(pd.read_csv('five_classes/results/test3_description_lenght_unnormalized.csv'))\n",
    "df5_4 = process_df(pd.read_csv('five_classes/results/test4_description_lenght_unnormalized.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d9c1a",
   "metadata": {},
   "source": [
    "### Plotting Hardness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74afbeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_2 = [df2_1, df2_2, df2_3, df2_4]\n",
    "dfs_3 = [df3_1, df3_2, df3_3, df3_4]\n",
    "dfs_5 = [df5_1, df5_2, df5_3, df5_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e9dfbd",
   "metadata": {},
   "source": [
    "#### r_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_2, hardness_measure='r_min', save_path='images/two_classes_r_min.png', show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab166aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_3, hardness_measure='r_min', save_path='images/three_classes_r_min.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571dc730",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_5, hardness_measure='r_min', show_plot=True), save_path='images/five_classes_r_min.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cac55",
   "metadata": {},
   "source": [
    "#### r_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ef948",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_2, hardness_measure='r_med', save_path='images/two_classes_r_med.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_3, hardness_measure='r_med', save_path='images/three_classes_r_med.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9420874",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_5, hardness_measure='r_med', save_path='images/five_classes_r_med.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62c6472",
   "metadata": {},
   "source": [
    "#### relative_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ce4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_2, hardness_measure='relative_position', save_path='images/two_classes_relative_position.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c6239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_3, hardness_measure='relative_position', save_path='images/three_classes_relative_position.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf132e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_5, hardness_measure='relative_position', save_path='images/five_classes_relative_position.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525bab8",
   "metadata": {},
   "source": [
    "#### pseudo_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_2, hardness_measure='pseudo_probability', save_path='images/two_classes_pseudo_probability.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d665db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_3, hardness_measure='pseudo_probability', save_path='images/three_classes_pseudo_probability.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714428b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_5, hardness_measure='pseudo_probability', save_path='images/five_classes_pseudo_probability.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8805caa",
   "metadata": {},
   "source": [
    "#### normalized_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_2, hardness_measure='normalized_entropy', save_path='images/two_classes_normalized_entropy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de22ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_3, hardness_measure='normalized_entropy', save_path='images/three_classes_normalized_entropy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_5, hardness_measure='normalized_entropy', save_path='images/five_classes_normalized_entropy.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f53d435",
   "metadata": {},
   "source": [
    "#### description_lenght_margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_2, hardness_measure='description_lenght_margin', save_path='images/two_classes_description_lenght_margin.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_3, hardness_measure='description_lenght_margin', save_path='images/three_classes_description_lenght_margin.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca38f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_5, hardness_measure='description_lenght_margin', save_path='images/five_classes_description_lenght_margin.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e6114",
   "metadata": {},
   "source": [
    "#### description_lenght_true_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f30446",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_2, hardness_measure='description_lenght_true_cost', save_path='images/two_classes_description_lenght_true_cost.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716945f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_3, hardness_measure='description_lenght_true_cost', save_path='images/three_classes_description_lenght_true_cost.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08395321",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_5, hardness_measure='description_lenght_true_cost', save_path='images/five_classes_description_lenght_true_cost.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449faec",
   "metadata": {},
   "source": [
    "#### kullback_leibler_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e533dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_2, hardness_measure='kullback_leibler_divergence', save_path='images/two_classes_kullback_leibler_divergence.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_3, hardness_measure='kullback_leibler_divergence', save_path='images/three_classes_kullback_leibler_divergence.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hardness(dfs=dfs_5, hardness_measure='kullback_leibler_divergence', show_plot=True)#, save_path='images/five_classes_kullback_leibler_divergence.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3976a4e5",
   "metadata": {},
   "source": [
    "### Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_cols = ['r_min', 'r_med', 'relative_position', 'pseudo_probability', 'normalized_entropy', 'description_lenght_margin', 'description_lenght_true_cost', 'kullback_leibler_divergence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_group_lineplot(dfs, measure_cols, group_name=None):\n",
    "    \n",
    "    difficulties = [1, 2, 3, 4]\n",
    "    \n",
    "    summary = []\n",
    "    for diff, df in zip(difficulties, dfs):\n",
    "        for col in measure_cols:\n",
    "            summary.append({\n",
    "                \"difficulty\": diff,\n",
    "                \"measure\": col,\n",
    "                \"mean\": df[col].mean(),\n",
    "                \"std\": df[col].std()\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    \n",
    "    # plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for col in measure_cols:\n",
    "        data = summary_df[summary_df[\"measure\"] == col]\n",
    "        \n",
    "        plt.plot(data[\"difficulty\"], data[\"mean\"], marker=\"o\", label=col)\n",
    "        plt.fill_between(\n",
    "            data[\"difficulty\"], \n",
    "            data[\"mean\"] - data[\"std\"], \n",
    "            data[\"mean\"] + data[\"std\"], \n",
    "            alpha=0.2\n",
    "        )\n",
    "    \n",
    "    plt.xticks(difficulties)\n",
    "    plt.xlabel(\"Hardness Level\")\n",
    "    plt.ylabel(\"Mean\")\n",
    "    plt.title(f\"Results - {group_name}\" if group_name else \"Results\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c41542",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_group_lineplot(dfs_2, measure_cols, group_name=\"Two Classes Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def plot_groups_side_by_side(groups: List[List[pd.DataFrame]], measure_cols: List[str], output_path: str = None):\n",
    "    difficulties = [1, 2, 3, 4]\n",
    "\n",
    "    summaries = []\n",
    "    for g_idx, dfs in enumerate(groups, start=1):\n",
    "        summary = []\n",
    "        for diff, df in zip(difficulties, dfs):\n",
    "            for col in measure_cols:\n",
    "                summary.append({\n",
    "                    \"group\": f\"Grupo {g_idx}\",\n",
    "                    \"difficulty\": diff,\n",
    "                    \"measure\": col,\n",
    "                    \"mean\": df[col].mean(),\n",
    "                    \"std\": df[col].std()\n",
    "                })\n",
    "        summaries.append(pd.DataFrame(summary))\n",
    "\n",
    "    n_measures = len(measure_cols)\n",
    "    fig, axes = plt.subplots(len(groups), n_measures, \n",
    "                             figsize=(3*n_measures, 4*len(groups)), \n",
    "                             sharey=True)\n",
    "    \n",
    "    if len(groups) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for g_idx, summary_df in enumerate(summaries):\n",
    "        for m_idx, col in enumerate(measure_cols):\n",
    "            ax = axes[g_idx][m_idx] if len(groups) > 1 else axes[m_idx]\n",
    "\n",
    "            data = summary_df[summary_df[\"measure\"] == col]\n",
    "            ax.plot(data[\"difficulty\"], data[\"mean\"], marker=\"o\", label=col, color=\"C0\")\n",
    "            ax.fill_between(\n",
    "                data[\"difficulty\"],\n",
    "                data[\"mean\"] - data[\"std\"],\n",
    "                data[\"mean\"] + data[\"std\"],\n",
    "                alpha=0.2,\n",
    "                color=\"C0\"\n",
    "            )\n",
    "            ax.set_xticks(difficulties)\n",
    "            \n",
    "            if g_idx == len(groups) - 1:  \n",
    "                ax.set_xlabel(\"Hardness Level\")\n",
    "            if m_idx == 0:\n",
    "                ax.set_ylabel(\"Mean\")\n",
    "            \n",
    "            ax.set_title(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if output_path:\n",
    "      plt.savefig(output_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1006604",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_groups_side_by_side([dfs_2, dfs_3, dfs_5], measure_cols, 'images/hardness_measures__mean_and_std.png')\n",
    "plot_groups_side_by_side([dfs_2, dfs_3, dfs_5], measure_cols[0:4], 'images/hardness_measures__mean_and_std_1.png')\n",
    "plot_groups_side_by_side([dfs_2, dfs_3, dfs_5], measure_cols[4:8], 'images/hardness_measures__mean_and_std_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_by_error(dfs: List[pd.DataFrame], measure_cols: List[str], output_path: str = None):\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    n_measures = len(measure_cols)\n",
    "    fig, axes = plt.subplots(2, (n_measures + 1)//2, figsize=(4*(n_measures//2), 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(measure_cols):\n",
    "        sns.violinplot(\n",
    "            data=df_all, x=\"is_error\", y=col, \n",
    "            ax=axes[i], palette=\"Set2\", inner=\"box\", hue=\"is_error\"\n",
    "        )\n",
    "        axes[i].set_title(f\"{col} distribution\")\n",
    "        axes[i].set_xlabel(\"is_error (0=ok, 1=error)\")\n",
    "        axes[i].set_ylabel(col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if output_path:\n",
    "      plt.savefig(output_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_by_error(dfs=dfs_2+dfs_3+dfs_5, measure_cols=measure_cols, output_path=\"images/hardness_measures__distribtion.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_heatmap(df: pd.DataFrame, title: str = \"corr\", output_path: str = None, figsize=(10, 8)):\n",
    "    corr = df.corr(method='kendall')\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, square=True, cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    if output_path:\n",
    "      plt.savefig(output_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aeff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = dfs_2 + dfs_3 + dfs_5\n",
    "df_all = pd.concat(all_dfs, ignore_index=True)\n",
    "df_all_hardness = df_all[measure_cols+[\"is_error\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e293a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_hardness.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f6cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_heatmap(df=df_all_hardness, title=\"correlation\", output_path=\"images/hardness_measures__correlation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931732d0",
   "metadata": {},
   "source": [
    "### PyHard vs HardnessMDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08eb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_1_PyHard = pd.read_csv('two_classes/test1_PyHard.csv').drop(columns=['X', 'Y', 'class'])\n",
    "df2_2_PyHard = pd.read_csv('two_classes/test2_PyHard.csv').drop(columns=['X', 'Y', 'class'])\n",
    "df2_3_PyHard = pd.read_csv('two_classes/test3_PyHard.csv').drop(columns=['X', 'Y', 'class'])\n",
    "df2_4_PyHard = pd.read_csv('two_classes/test4_PyHard.csv').drop(columns=['X', 'Y', 'class'])\n",
    "\n",
    "df3_1_PyHard = pd.read_csv('three_classes/test1_PyHard.csv').drop(columns=['X', 'Y', 'class'])\n",
    "df3_2_PyHard = pd.read_csv('three_classes/test2_PyHard.csv').drop(columns=['X', 'Y', 'class'])\n",
    "df3_3_PyHard = pd.read_csv('three_classes/test3_PyHard.csv').drop(columns=['X', 'Y', 'class'])\n",
    "df3_4_PyHard = pd.read_csv('three_classes/test4_PyHard.csv').drop(columns=['X', 'Y', 'class'])\n",
    "\n",
    "df5_1_PyHard = pd.read_csv('five_classes/test1_PyHard.csv').drop(columns=['X', 'Y', 'class'])\n",
    "df5_2_PyHard = pd.read_csv('five_classes/test2_PyHard.csv').drop(columns=['X', 'Y', 'class'])\n",
    "df5_3_PyHard = pd.read_csv('five_classes/test3_PyHard.csv').drop(columns=['X', 'Y', 'class'])\n",
    "df5_4_PyHard = pd.read_csv('five_classes/test4_PyHard.csv').drop(columns=['X', 'Y', 'class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_PyHard = pd.concat([df2_1_PyHard, df2_2_PyHard, df2_3_PyHard, df2_4_PyHard, df3_1_PyHard, df3_2_PyHard, df3_3_PyHard, df3_4_PyHard, df5_1_PyHard, df5_2_PyHard, df5_3_PyHard, df5_4_PyHard], \n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6189792",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_all_hardness, df_all_PyHard], axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hardnessmdl_instancehardness = pd.concat([df_all_hardness, df_all_PyHard.instance_hardness], axis=1)\n",
    "plot_correlation_heatmap(df=df_hardnessmdl_instancehardness, title=\"correlation\", output_path=\"images/hardness_measures__pyhard_instance_hardness.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hardnessmdl_pyhard = pd.concat([df_all_hardness, df_all_PyHard], axis=1)\n",
    "plot_correlation_heatmap(df=df_hardnessmdl_pyhard, title=\"correlation\", output_path=\"images/hardness_measures__pyhard_correlation.png\", figsize=(20, 16))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
